

## Related Work

A common approach to privacy in IoT systems is to leverage cryptographic protection or local (edge) processing to avoid exposing raw data. Techniques like homomorphic encryption, federated learning, and differential privacy have been explored for smart home data protection.\cite{guptaPrivacyPreservingGenerativeAI2025,abdmeziemLeveragingIoTLLM2025,chronisPrivacyPreservingEnergyRecommendations2024,chenPrivacyAwareSplitFederated2025,chowdhuryIdentifyingAddressingUserlevel2025}

物联网系统中保障隐私的常用方法是利用加密保护或本地（边缘）处理，避免原始数据暴露。同态加密、联邦学习和差分隐私等技术已被用于智能家居数据保护的相关研究。

These methods can secure or obfuscate sensitive information, but they often come with significant trade-offs. For instance, fully encrypting data for cloud processing incurs heavy computational overhead and prevents direct semantic interpretation by an LLM, limiting the usefulness of cloud intelligence. Differential privacy, while adding statistical noise to limit data disclosure, may degrade the fidelity of commands or sensor inputs, hindering accurate automation responses. Federated learning keeps training data on the device but primarily addresses model training rather than real-time command privacy, and it requires a large cohort of devices to be effective. Given these constraints, researchers have emphasized edge-only computation in smart homes to keep data local. For example, Hewitt and Cunningham propose a cloud-free voice assistant that processes commands entirely on local hardware to ensure no user audio or device data leaves the home.\cite{hewittPrivacyPreservingVoiceControl2024}

这些方法能够保护或模糊敏感信息，但通常存在显著的权衡取舍。例如，为云处理而对数据进行全加密会产生巨大的计算开销，且阻碍大语言模型对数据的直接语义解读，进而限制云智能的实用性。差分隐私通过添加统计噪声来限制数据泄露，但可能降低指令或传感器输入的真实性，影响自动化响应的准确性。联邦学习将训练数据保留在设备端，但主要解决模型训练问题而非实时指令的隐私保护，且需要大量设备参与才能生效。鉴于这些限制，研究人员强调智能家居应采用纯边缘计算，确保数据本地化。例如，休伊特（Hewitt）和坎宁安（Cunningham）提出了一种无云语音助手，该助手完全通过本地硬件处理指令，确保用户音频和设备数据不会离开家庭环境 \cite {hewittPrivacyPreservingVoiceControl2024}。

This edge-based strategy avoids external data exposure and thus bolsters privacy. However, purely local solutions must either sacrifice the powerful reasoning of large cloud-based LLMs or depend on high-end hardware in the home. In practice, resource-constrained IoT devices often cannot run full-scale LLMs, leading to diminished functionality or latency issues when using smaller models. Edge computing frameworks integrating LLMs highlight this tension: deploying models on local gateways improves privacy and reduces latency, but there is a clear accuracy vs. efficiency trade-off as model size shrinks. In summary, while encryption and edge computation can mitigate data leakage, they tend to undermine the semantic richness and convenience that centralized LLMs offer, either by stripping context through encryption or by limiting processing to less capable local models. 

这种基于边缘的策略避免了外部数据暴露，从而增强了隐私保护。然而，纯本地解决方案要么必须牺牲云端大型语言模型（LLMs）的强大推理能力，要么就得依赖家庭中的高端硬件。实际应用中，资源受限的物联网设备往往无法运行完整规模的大型语言模型，使用小型模型时则会导致功能减弱或延迟问题。集成了大型语言模型的边缘计算框架凸显了这一矛盾：在本地网关部署模型能提升隐私性并降低延迟，但随着模型规模缩小，准确性与效率之间的权衡会愈发明显。总之，尽管加密和边缘计算能减轻数据泄露风险，但它们往往会削弱集中式大型语言模型所提供的语义丰富性和便捷性 —— 要么通过加密剥离上下文，要么将处理限制在性能较弱的本地模型上。

To balance privacy with the capabilities of AI, some emerging approaches turn to smaller-scale models and data modification techniques. One strategy is to deploy or fine-tune small language models (SLMs) on-premise, which can operate without sending data off-site. For instance, Huang et al. develop HomeLLaMA, an on-device assistant using a tailored lightweight LLM, combined with a cloud-backed component for non-sensitive queries.\cite{huangPrivacyPreservingPersonalizedSmart2025}

为平衡隐私与人工智能的功能，一些新兴方法转向了小型模型和数据修改技术。一种策略是在本地部署或微调小型语言模型（SLMs），这类模型无需将数据发送至异地即可运行。例如，黄（Huang）等人开发了 HomeLLaMA—— 一款基于定制轻量化大型语言模型（LLM）的设备端助手，同时结合云端组件处理非敏感查询 \cite {huangPrivacyPreservingPersonalizedSmart2025}。

By learning from a larger cloud model’s knowledge, the local HomeLLaMA can handle many personalized tasks, while a module called PrivShield selectively forwards only less-sensitive requests to the cloud as a privacy safeguard. This hybrid design improves privacy over a purely cloud-based assistant, yet it still must carefully decide what is “safe” to share, and the local model may lag behind state-of-the-art performance for complex queries. Other work has shown the promise of fine-tuning compact models (e.g., T5 or Flan-T5 transformers) on smart home data so they can be run on-site. Chowdhury et al. argue that “smaller models are more feasible for deployment in … privacy-sensitive environments like smart homes” since large models like GPT-4 or Gemini demand extensive resources and require users to trust external servers.\cite{chowdhuryIdentifyingAddressingUserlevel2025} 

通过借鉴云端大型模型的知识，本地的 HomeLLaMA 能够处理多项个性化任务，而一个名为 PrivShield 的模块会选择性地仅将敏感度较低的请求转发至云端，作为隐私保障措施。这种混合设计比纯云端助手更能提升隐私保护水平，但仍需谨慎判断哪些信息 “可安全共享”，且本地模型在处理复杂查询时，性能可能落后于当前最先进的水平。其他研究表明，在智能家居数据上微调紧凑型模型（如 T5 或 Flan-T5 转换器）具有应用前景，可使其在本地运行。乔杜里（Chowdhury）等人认为，“小型模型更适合部署在…… 智能家居等隐私敏感环境中”，因为 GPT-4 或 Gemini 等大型模型需要大量资源，且要求用户信任外部服务器 \cite {chowdhuryIdentifyingAddressingUserlevel2025}。

Their system trains a question-answering model for home security help by curating public Q\&A data and even injecting synthetic data to broaden the model’s knowledge without exposing real user logs. **Synthetic data generation and augmentation are indeed used as privacy-preserving techniques – by training or evaluating AI on artificial yet statistically similar data, one can avoid using sensitive real-world records.** However, synthetic data may not capture all the nuances of a specific user’s environment, and excessive reliance on it can impact the accuracy or relevance of the model’s responses. Similarly, adding noise via differential privacy or generalization can protect individual data points but at the cost of reduced precision in automation (e.g., slightly perturbing a thermostat setting might protect exact habits but also yield less exact climate control). In sum, lighter models and data alteration strategies offer partial solutions – they mitigate the need to send raw data to the cloud and reduce the risk of leakage, but they often come with compromises in model capability or fidelity of service.他们的系统通过整理公开问答（Q&A）数据，甚至注入合成数据来扩展模型知识，无需暴露真实用户日志，从而训练出一款用于家庭安全求助的问答模型。合成数据生成与增强确实被用作隐私保护技术 —— 通过在人工生成但统计特征相似的数据上训练或评估人工智能，可避免使用敏感的真实世界记录。然而，合成数据可能无法捕捉特定用户环境的所有细微特征，过度依赖会影响模型响应的准确性或相关性。同样，通过差分隐私添加噪声或进行泛化处理能保护单个数据点，但会以降低自动化精度为代价（例如，轻微干扰恒温器设置可能保护用户的确切习惯，但也会导致气候控制不够精确）。总之，轻量化模型和数据修改策略仅提供部分解决方案 —— 它们减少了向云端发送原始数据的需求，降低了泄露风险，但往往会在模型性能或服务真实性方面做出妥协。

In light of these limitations, the proposed Semantic Variable Substitution (SVR) architecture takes a different approach to privacy preservation in LLM-enabled IoT. Rather than encrypting or removing private information (which, as discussed, can break the query’s meaning or limit functionality), SVR replaces sensitive attributes with semantically equivalent placeholders. This design ensures that an agent can still leverage powerful LLM reasoning on the transformed input without ever seeing the real private data. Moreover, SVR can be integrated into the IoT assistant’s reasoning loop – meaning privacy enforcement is built into the agent’s understanding and action planning, instead of being an external add-on. Our SVR-based solution improves upon these works by bridging the privacy-utility divide: it empowers large-scale language models to automate smart homes while systematically preventing the exposure of identifiable personal data. In contrast to earlier edge-only or small-model approaches, the SVR agent can tap into cloud-level AI capabilities when needed, yet the sensitive particulars (e.g., user names, precise device identifiers, exact timestamps) are swapped out for abstracted tokens. As a result, the architecture offers a novel blend of strong privacy protection and semantic preservation, enabling robust smart home automation without the shortcomings of encryption, coarse anonymization, or all-or-nothing cloud reliance noted in previous work.鉴于这些局限性，本文提出的语义变量替换（SVR）架构，针对支持大语言模型（LLM）的物联网系统，采用了一种不同的隐私保护方法。该架构不采用加密或删除隐私信息的方式（如前所述，这些方式会破坏查询的含义或限制功能），而是用语义等效的占位符替换敏感属性。这种设计确保智能体仍能基于转换后的输入，利用大语言模型的强大推理能力，同时永远不会接触到真实的隐私数据。此外，语义变量替换（SVR）可集成到物联网助手的推理循环中 —— 这意味着隐私保护机制已内置到智能体的理解和行动规划过程中，而非外部附加组件。本文基于语义变量替换（SVR）的解决方案，通过弥合隐私与实用性之间的鸿沟，对现有研究进行了改进：它既赋能大型语言模型实现智能家居自动化，又系统地防止可识别个人数据的暴露。与早期的纯边缘或小型模型方法不同，基于语义变量替换（SVR）的智能体可在需要时利用云端级别的人工智能功能，同时将敏感细节（如用户名、精确设备标识符、确切时间戳）替换为抽象化令牌。因此，该架构创新性地融合了强大的隐私保护与语义保留功能，实现了稳健的智能家居自动化，同时避免了现有研究中提到的加密、粗略匿名化或非全即无的云端依赖等缺陷。



## introduction

With the rise of large language models, their integration into Internet of Things systems has become a significant trend to enhance intelligence and autonomy. LLMs possess powerful natural language understanding and reasoning capabilities, enabling more intuitive human-computer interaction and autonomous decision-making within IoT environments. However, the introduction of LLMs into IoT systems also brings new challenges, particularly in the realms of privacy and security. The most pressing concern is the risk of user data privacy leakage. Many IoT devices are deployed in private living spaces or sensitive environments (e.g., homes, healthcare settings), where they collect large amounts of highly personal and private data. Sending this raw data to third-party LLM cloud services for analysis or control raises significant privacy risks, including unauthorized access, data breaches, or misuse of information. Furthermore, LLMs themselves could inadvertently expose private data learned during training or leak sensitive details from prompts during the response generation process. Given the often opaque data processing practices of LLM service providers, transmitting data to the cloud may also conflict with data protection regulations such as the GDPR. This issue has led some countries, such as Italy, and major corporations, including Samsung, Amazon, and Apple, to prohibit employees from inputting sensitive data into third-party LLMs, underscoring the severity of the problem.

随着大型语言模型的兴起，将其整合到物联网系统中已成为提升智能性与自主性的重要趋势。大型语言模型具备强大的自然语言理解和推理能力，能够在物联网环境中实现更直观的人机交互与自主决策。

然而，将大型语言模型引入物联网系统也带来了新的挑战，尤其在隐私与安全领域。最迫切的问题是用户数据隐私泄露风险。许多物联网设备部署于私人生活空间或敏感环境（如家庭、医疗场所），会收集大量高度个人化的隐私数据。将这些原始数据发送至第三方大型语言模型云服务进行分析或控制，会引发严重的隐私风险，包括未授权访问、数据泄露或信息滥用。

此外，大型语言模型本身可能在训练过程中不慎暴露所学的隐私数据，或在生成响应时泄露提示词中的敏感细节。鉴于大型语言模型服务提供商的数据处理流程往往不够透明，向云端传输数据还可能违反《通用数据保护条例》（GDPR）等数据保护法规。这一问题已导致意大利等部分国家，以及三星、亚马逊、苹果等大型企业禁止员工向第三方大型语言模型输入敏感数据，凸显了该问题的严重性。

Several approaches have been explored to address LLM data privacy concerns on IoT devices. One approach is to deploy lightweight LLM models locally on IoT devices or at the edge, thereby avoiding the transmission of sensitive data. However, LLMs are typically large and computation-intensive, making it difficult to run a full model on resource-constrained IoT devices or home gateways due to cost and performance limitations. Another solution is to anonymize or encrypt the data, using techniques such as masking sensitive fields, pseudonymization, or advanced methods like homomorphic encryption and secure multiparty computation to ensure the cloud-based LLM cannot directly access raw data. While homomorphic encryption offers strong privacy guarantees, it remains highly inefficient. More efficient solutions often lack strong privacy guarantees or require modifications to the existing LLM service interfaces. 

已有多种方法被用于解决物联网设备上大型语言模型（LLMs）的数据隐私问题。一种方法是在物联网设备本地或边缘端部署轻量化大型语言模型，从而避免敏感数据的传输。然而，大型语言模型通常体积庞大且计算密集，受成本和性能限制，难以在资源受限的物联网设备或家庭网关上运行完整模型。

另一种解决方案是对数据进行匿名化或加密处理，采用诸如敏感字段屏蔽、假名化等技术，或同态加密、安全多方计算等先进方法，确保云端大型语言模型无法直接访问原始数据。尽管同态加密能提供强有力的隐私保障，但效率仍极低。更高效的解决方案往往缺乏可靠的隐私保障，或需要修改现有的大型语言模型服务接口。

### 参考

The proliferation of Internet of Things (IoT) devices in smart homes has brought unprecedented convenience to daily life. With recent advances in artificial intelligence, particularly large language models (LLMs) such as GPT-4, interactions in smart homes are being significantly transformed. These models have demonstrated exceptional capabilities for understanding natural language and reasoning, with great potential to rev olutionize human-computer interaction in such environments. Researchers are beginning to explore the symbiotic relation ship between LLMs and IoT devices, demonstrating that LLMs can facilitate more context-aware, natural interactions across various smart home scenarios. While these technologies promise to make users’ lives more convenient, they also increase the risk of privacy breaches [1].智能家居中物联网（IoT）设备的普及，为日常生活带来了前所未有的便利。随着人工智能领域的最新进展，尤其是 GPT-4 等大型语言模型（LLMs）的出现，智能家居中的交互方式正发生显著变革。这些模型展现出卓越的自然语言理解与推理能力，有望彻底革新此类环境中的人机交互模式。

研究人员已开始探索大型语言模型与物联网设备之间的共生关系，并证实大型语言模型能够在各类智能家居场景中，促成更具上下文感知能力、更自然的交互。尽管这些技术有望为用户生活带来更多便利，但也增加了隐私泄露的风险 [1]。